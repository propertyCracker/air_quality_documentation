
# Evaluation

The LLM is evaluated using:

## Metrics
- Response accuracy  
- Relevance to environmental data  
- Latency and performance  
- Error rate on structured queries  

## Testing Scenarios
- Real streaming alerts  
- Historical air-quality analysis  
- Extreme pollution events  

# Deployment

The LLM module can be deployed in multiple environments:

## Deployment Methods
- **Azure Container Apps**  
- **Streamlit app backend**  
- **Serverless Functions for lightweight requests**  

## Requirements
- Access to Synapse endpoints  
- Access to IoT Hub or processed data  
- Network permissions to Azure services  

```
{
  "introduction": "Introduction",
  "model-selection": "Model Selection",
  "prompt-design": "Prompt Design",
  "evaluation": "Evaluation",
  "deployment": "Deployment"
}
```


## Best Practices
- Always include relevant data  
- Use clear instructions  
- Prevent hallucination by constraining context  
